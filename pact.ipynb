{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import colorsys\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "from captum.attr import (\n",
    "    FeatureAblation, \n",
    "    LLMAttribution, \n",
    "    TextTokenInput, \n",
    "    LLMAttributionResult,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set API-KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '' # your openai key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select model you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4o'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(json_path: str = 'pact.json'):\n",
    "    with open(json_path, 'r') as file:\n",
    "        question_all = json.load(file)\n",
    "    return question_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_all = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logprob_chat(input_messages, model_name: str, output_token: str):\n",
    "    \"\"\"\n",
    "    Get the log probability of a specific output token from a chat model.\n",
    "\n",
    "    This function sends a chat request to the OpenAI model and retrieves the log probabilities of the output tokens.\n",
    "    It checks if the target output token is within the top 20 tokens, and if so, returns its log probability.\n",
    "    If the token is not found in the top 20 tokens, a fallback value is returned based on the experience.\n",
    "    The function handles retries in case of errors during the request.\n",
    "\n",
    "    Args:\n",
    "        input_messages (list): A list of messages in the conversation. Each message is a dictionary containing 'role' and 'content' keys.\n",
    "        model_name (str): The name of the model to use (e.g. 'gpt-3.5-turbo').\n",
    "        output_token (str): The target output token for which to retrieve the log probability.\n",
    "\n",
    "    Returns:\n",
    "        np.array: A numpy array containing the log probability of the target output token or a fallback value if the token is not found in the top 20.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    res = [] \n",
    "    max_retries = 5 \n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=input_messages,\n",
    "                max_completion_tokens=1,\n",
    "                logprobs=True,\n",
    "                top_logprobs=20\n",
    "            )\n",
    "            if response.choices[0].logprobs.content[0].top_logprobs: \n",
    "                logprob_dict = {}\n",
    "                for logprobInstance in response.choices[0].logprobs.content[0].top_logprobs:\n",
    "                    logprob_dict[logprobInstance.token] = logprobInstance.logprob\n",
    "                if output_token in logprob_dict:\n",
    "                    print(f\"Yes! target token '{output_token}' is in top20: {logprob_dict}\")\n",
    "                    res.append(logprob_dict[output_token])\n",
    "                else:  # When the target token is not in the top 20 tokens, a value based on experience is taken. This can be optimized through the probability distribution of the vocabulary output by the large language model.  \n",
    "                    print(f\"Oh no! target token '{output_token}' is not in top20: {logprob_dict}\")\n",
    "                    res.append(min(logprob_dict.values()) * 2)\n",
    "                break \n",
    "            else:\n",
    "                raise ValueError(\"logprobs.top_logprobs[0] is None\")  # Manually trigger an exception to retry\n",
    "\n",
    "        except (AttributeError, IndexError, TypeError, ValueError) as e:\n",
    "            print(f\"Error encountered: {e}, retrying... ({retries + 1}/{max_retries})\")\n",
    "            retries += 1 \n",
    "    if retries == max_retries:\n",
    "        print(\"Max retries reached, using default value.\")\n",
    "        res.append(-10000)  # If the maximum retries are reached, use a default value\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "def featureAbalationSegmentChat(input_messages, model_name: str, target=None, base_segment=''):\n",
    "    \"\"\"\n",
    "    Perform feature ablation on a segment-based level for attribution analysis.\n",
    "\n",
    "    This function performs feature ablation by replacing each segment in the input conversation with a base segment \n",
    "    (e.g., empty string or predefined text) and computes the difference in log probabilities before and after the modification \n",
    "    for each segment. It returns the attribution results for each token and the total attribution score for the entire input.\n",
    "\n",
    "    Args:\n",
    "        input_messages (list): A list of message dictionaries, each containing 'role' and 'content' keys.\n",
    "        model_name (str): The name of the model to use (e.g., 'gpt-3.5-turbo').\n",
    "        target (str, optional): The target output token for which the attribution is calculated. If `None`, the function \n",
    "                                 retrieves the target token from the model response.\n",
    "        base_segment (str, optional): The base segment to replace each segment in the input. Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        LLMAttributionResult: An object containing:\n",
    "                seq_attr: A tensor with the token-level attribution scores.\n",
    "                attr: A tensor with the segment-level attribution scores.\n",
    "                input_tokens: A list of input tokens.\n",
    "                target: The target output token.\n",
    "    \"\"\"\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    input_segment_length = len(input_messages)\n",
    "\n",
    "    if target is None:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=input_messages,\n",
    "            max_completion_tokens=1\n",
    "        )\n",
    "        target = response.choices[0].message.content\n",
    "\n",
    "    attr = np.zeros((1, input_segment_length))\n",
    "    ref = get_logprob_chat(input_messages, model_name, target)\n",
    "\n",
    "    for i in range(input_segment_length):\n",
    "        modified_messages = [dict(item) for item in input_messages]\n",
    "        modified_messages[i]['content'] = base_segment\n",
    "        attr[:, i] = ref - get_logprob_chat(modified_messages, model_name, target)\n",
    "\n",
    "    seq_attr = np.sum(attr, axis=0)\n",
    "\n",
    "    return LLMAttributionResult(torch.tensor(seq_attr), torch.tensor(attr), [item['content'] for item in input_messages], [target], target=None)\n",
    "\n",
    "def featureAbalationChat(input_messages, model_name: str, target=None, base_token=''):\n",
    "    \"\"\"\n",
    "    Perform feature ablation on a token-based level for attribution analysis.\n",
    "\n",
    "    This function performs feature ablation by modifying individual tokens in the input conversation and computing \n",
    "    the difference in log probabilities before and after the modification for each token. It returns two attribution results:\n",
    "    one for token-level visualization and another for segment-level visualization, averaged by token attribution within the segment.\n",
    "\n",
    "    Args:\n",
    "        input_messages (list): A list of message dictionaries, each containing 'role' and 'content' keys.\n",
    "        model_name (str): The name of the model to use (e.g. 'gpt-3.5-turbo').\n",
    "        target (str, optional): The target output token for which the attribution is calculated. If `None`, the function \n",
    "                                 retrieves the target token from the model response.\n",
    "        base_token (str, optional): The base token to replace individual tokens in the input. Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two `LLMAttributionResult` objects:\n",
    "            - The first result contains token-level attribution scores.\n",
    "            - The second result contains segment-level attribution scores, averaged by token attribution within each segment.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    input_segment_length = len(input_messages)\n",
    "    input_token_id_list_list = [enc.encode(item['content']) for item in input_messages]  # List of lists of token ids\n",
    "    input_token_list_list = []  # List of lists of tokens\n",
    "    input_token_list = []  # Flattened list of tokens\n",
    "\n",
    "    for token_id_list_partial in input_token_id_list_list:\n",
    "        token_list_partial = [enc.decode([token_id]) for token_id in token_id_list_partial]\n",
    "        input_token_list_list.append(token_list_partial)\n",
    "        input_token_list += token_list_partial\n",
    "    input_token_length = len(input_token_list)\n",
    "\n",
    "    if target is None:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=input_messages,\n",
    "            max_completion_tokens=1\n",
    "        )\n",
    "        target = response.choices[0].message.content\n",
    "\n",
    "    attr = np.zeros((1, input_token_length))\n",
    "    ref = get_logprob_chat(input_messages, model_name, target)\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(input_segment_length):\n",
    "        modified_messages = [dict(item) for item in input_messages]\n",
    "        input_token_list_partial = input_token_list_list[i]\n",
    "        for j in range(len(input_token_list_partial)):  # j is the index of the token to be ablated\n",
    "            temp_prompt = ''\n",
    "            for k in range(len(input_token_list_partial)):\n",
    "                if j == k:\n",
    "                    temp_prompt += base_token\n",
    "                else:\n",
    "                    temp_prompt += input_token_list_partial[k]\n",
    "            modified_messages[i]['content'] = temp_prompt\n",
    "            attr[:, cnt] = ref - get_logprob_chat(modified_messages, model_name, target)\n",
    "            cnt += 1\n",
    "        seq_attr = np.sum(attr, axis=0)\n",
    "    attr_segment = np.zeros((1, input_segment_length))\n",
    "\n",
    "    start = 0\n",
    "    for i in range(input_segment_length):\n",
    "        end = start + len(input_token_list_list[i])\n",
    "        attr_segment[0, i] = np.average(attr[0, start:end])\n",
    "        start = end\n",
    "    seq_attr_segment = np.sum(attr_segment, axis=0)\n",
    "    \n",
    "    return (LLMAttributionResult(torch.tensor(seq_attr), torch.tensor(attr), input_token_list, [target], target=None),\n",
    "            LLMAttributionResult(torch.tensor(seq_attr_segment), torch.tensor(attr_segment), [item['content'] for item in input_messages], [target], target=None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paint the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_new_visualization(AttributionResult: LLMAttributionResult, model_name: str, save: bool = False) -> Image:\n",
    "\n",
    "    \"\"\"\n",
    "    Draw the attribution visualization for a given model's results.\n",
    "\n",
    "    This function generates a visualization that highlights the contribution of different tokens in the input \n",
    "    sequence based on their attribution values. It uses color-coded backgrounds to represent the magnitude of \n",
    "    attribution, with positive values showing a light green color and negative values showing a light yellow \n",
    "    color. It also handles token formatting and visualization for different model types, such as Llama and 4o.\n",
    "\n",
    "    Args:\n",
    "        AttributionResult (LLMAttributionResult): The attribution result object containing the attribution values \n",
    "                                                   and input tokens.\n",
    "        model_name (str): The name of the model (e.g., 'Llama3.1-8b-Instruct', 'gpt-4o').\n",
    "        save (bool, optional): Whether to save the generated image to a file. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        Image: The generated image with token-level attribution visualization.\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "\n",
    "    data = AttributionResult.seq_attr.cpu().numpy()\n",
    "\n",
    "    n_inputs = len(AttributionResult.input_tokens)\n",
    "    for i in range(n_inputs):\n",
    "\n",
    "        if data[i] > 0:\n",
    "            # Light green: HSV conversion: Hue=90 degrees, Value=90%, Saturation increases with data[i]\n",
    "            hue = 90\n",
    "            value = 0.9\n",
    "            saturation = min(0.2 + 0.6 * data[i], 1)\n",
    "            bg_color = colorsys.hsv_to_rgb(hue / 360, saturation, value)\n",
    "            bg_color_rgb = tuple(int(c * 255) for c in bg_color) \n",
    "            text_parts.append({'text': f'{AttributionResult.input_tokens[i].replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")}', 'bg_color': bg_color_rgb})           \n",
    "        elif data[i] < 0:\n",
    "            # Light yellow: HSV conversion: Hue=50 degrees, Value=100%, Saturation increases with data[i]\n",
    "            hue = 60\n",
    "            value = 1.0\n",
    "            saturation = min(0.2 - 1 * data[i], 1)\n",
    "            bg_color = colorsys.hsv_to_rgb(hue / 360, saturation, value)\n",
    "            bg_color_rgb = tuple(int(c * 255) for c in bg_color)\n",
    "            text_parts.append({'text': f'{AttributionResult.input_tokens[i].replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")}', 'bg_color': bg_color_rgb})\n",
    "        else:\n",
    "            text_parts.append({'text': f'{AttributionResult.input_tokens[i].replace(\"Ġ\", \" \").replace(\"Ċ\", \"\\n\")}', 'bg_color': (255, 255, 255)})\n",
    "\n",
    "    if 'Llama' in model_name:\n",
    "        image = create_colored_text_image(text_parts=text_parts[1:])\n",
    "    elif '4o' in model_name:\n",
    "        image = create_colored_text_image(text_parts=text_parts)\n",
    "    if save:\n",
    "        image.save(\"attribute_colored_text_image.png\")\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "def create_colored_text_image(\n",
    "        text_parts: list, \n",
    "        font_path: str = \"Arial.ttf\", \n",
    "        font_size: int = 40,\n",
    "        background_color: str = \"white\",\n",
    "        max_width: int = 785,\n",
    "        line_spacing: int = 15\n",
    "        ) -> Image:\n",
    "    \"\"\"\n",
    "    Create a contribution visualization image, similar to the highlighted background text shown.\n",
    "\n",
    "    Args:\n",
    "        text_parts: List, each element is a dictionary containing \"text\" and \"bg_color\" keys, representing the text content and corresponding background color.\n",
    "        font_path: Font file path (default is Avenir Next font).\n",
    "        font_size: Font size.\n",
    "        background_color: Image background color.\n",
    "        max_width: Maximum width for each line; words will wrap if exceeded.\n",
    "        line_spacing: Space between lines.\n",
    "\n",
    "    Returns:\n",
    "        Image object: Contribution visualization image with specified styles.\n",
    "    \"\"\"\n",
    "    # Use the specified font\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    \n",
    "    # Initialize variables\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_width = 0\n",
    "\n",
    "    # Calculate line breaks\n",
    "    for part in text_parts:\n",
    "        text = part[\"text\"]\n",
    "        bg_color = part[\"bg_color\"]\n",
    "\n",
    "        # Process word by word\n",
    "        for word in text.split(' '):\n",
    "            word_width = font.getbbox(word)[2]\n",
    "            \n",
    "            # Check if it exceeds the line width\n",
    "            if current_width + word_width > max_width:\n",
    "                lines.append(current_line)\n",
    "                current_line = []\n",
    "                current_width = 0\n",
    "            \n",
    "            # Add word to current line\n",
    "            current_line.append({\"text\": word, \"bg_color\": bg_color})\n",
    "            current_width += word_width + font.getbbox(' ')[2]  # Include space width\n",
    "\n",
    "        # Check for line breaks\n",
    "        if '\\n' in text:\n",
    "            lines.append(current_line)\n",
    "            current_line = []\n",
    "            current_width = 0\n",
    "\n",
    "    # Add the last line\n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "\n",
    "    # Calculate image height\n",
    "    image_height = len(lines) * (font_size + line_spacing) + 20\n",
    "    image = Image.new(\"RGBA\", (max_width + 20, image_height), background_color)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Draw text and background line by line\n",
    "    y = 10\n",
    "    for line in lines:\n",
    "        x = 10  # Starting x-coordinate for each line\n",
    "        for part in line:\n",
    "            text = part[\"text\"]\n",
    "            bg_color = part[\"bg_color\"]\n",
    "            \n",
    "            # Get text width and height, then draw background rectangle\n",
    "            text_width, text_height = font.getbbox(text)[2], font.getbbox(text)[3]\n",
    "            draw.rectangle([(x, y), (x + text_width, y + text_height)], fill=bg_color)\n",
    "            \n",
    "            # Draw the text\n",
    "            draw.text((x, y), text, font=font, fill=\"black\")  # Default text color is black\n",
    "            x += text_width + font.getbbox(' ')[2]  # Update x position, including space width\n",
    "        y += font_size + line_spacing  # Update y position\n",
    "\n",
    "    return image\n",
    "\n",
    "def create_contribution_visualization(model_name: str, question_all: dict) -> Image:\n",
    "    \"\"\"\n",
    "    Draw the attribution visualization.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name,\n",
    "        question_all (dict): The questions to be visualized,\n",
    "        type (str): The type of visualization filter,\n",
    "        dataset_type (str): The type of dataset used.\n",
    "\n",
    "    Returns:\n",
    "        image: List of visualized images\n",
    "    \"\"\"\n",
    "\n",
    "    image = []\n",
    "\n",
    "    if \"Llama\" in model_name:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).cuda()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    for question in question_all:\n",
    "\n",
    "        first_question = question['question'] + '\\n'\n",
    "        first_answer = question['answer'] + '\\n'\n",
    "        refinement = question['refinement']\n",
    "        target = question['target']\n",
    "\n",
    "        messages = [{'role': 'user', 'content': first_question}, {'role': 'assistant', 'content': first_answer}, {'role': 'user', 'content': refinement}]\n",
    "        \n",
    "        \n",
    "        if 'gpt' in model_name:\n",
    "            attr_res, _ = featureAbalationChat(messages, model_name, target=target)\n",
    "\n",
    "        elif 'Llama' in model_name:\n",
    "            eval_prompt = tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "            inp = TextTokenInput(\n",
    "                eval_prompt, \n",
    "                tokenizer,\n",
    "                skip_tokens=[1],  # 跳过特殊起始token <s>\n",
    "            )\n",
    "            \n",
    "            fa = FeatureAblation(model)\n",
    "            llm_attr = LLMAttribution(fa, tokenizer)\n",
    "            attr_res = llm_attr.attribute(inp, target=target)\n",
    "\n",
    "        image.append(plot_new_visualization(AttributionResult=attr_res, model_name=model_name, save=True))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_contribution_visualization(model_name=model_name,question_all=question_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reflexion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
